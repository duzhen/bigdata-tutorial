{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bigdata-LA3 tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Frequent itemsets\n",
    "### pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext('local','bigdata-LA3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "datalist = [[1,2,5],[1,2,3,5],[1,2],[1,4,5],[1,3,5],[2,3,4],[2,4],[2,3]]\n",
    "rdd = sc.parallelize(datalist)\n",
    "rdd = rdd.zipWithIndex()\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"bigdata-LA3\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_row(row):\n",
    "    from pyspark.sql import Row\n",
    "    \n",
    "    id = row[1]\n",
    "    return Row(id=int(id), items=row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRDD = rdd.map(get_row)\n",
    "df = spark.createDataFrame(dfRDD)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FPGrowth algorithm\n",
    "[FP-Growth](http://spark.apache.org/docs/latest/ml-frequent-pattern-mining.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.fpm import FPGrowth\n",
    "\n",
    "fpGrowth = FPGrowth(itemsCol=\"items\", minSupport=0.5, minConfidence=0.5)\n",
    "model = fpGrowth.fit(df)\n",
    "\n",
    "# Display frequent itemsets.\n",
    "freq = model.freqItemsets\n",
    "freq.show()\n",
    "\n",
    "# Display generated association rules.\n",
    "asso = model.associationRules\n",
    "asso.show()\n",
    "# transform examines the input items against all the association rules and summarize the\n",
    "# consequents as prediction\n",
    "model.transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalItems = df.count()\n",
    "freq_df = freq.filter(size(freq.items)==1)\n",
    "frequent = asso.join(freq_df, asso.consequent == freq_df.items)\n",
    "# interest\n",
    "interset = frequent.withColumn(\"interest\", abs(frequent.confidence-frequent.freq/totalItems)) \\\n",
    "            .select(\"antecedent\",\"consequent\", \"confidence\", \"items\", \"freq\", \"interest\")\n",
    "interset.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clustering\n",
    "\n",
    "[K-means](https://github.com/glatard/big-data-analytics-course/tree/master/kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getItemsVector(rdd):\n",
    "    items = rdd[0]\n",
    "    basket = rdd[1]\n",
    "    output = []\n",
    "    for item in range(1,6):\n",
    "        output.append((item, (basket, int(item in items))))\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#items and baskets\n",
    "print(rdd.collect())\n",
    "print(\"\\nitems in baskets\\n(item, [(basket, T/F)...])\")\n",
    "data = rdd.flatMap(getItemsVector).groupByKey().map(lambda x : [x[0], list(x[1])])\n",
    "data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def getCoordinate(data):\n",
    "    return tuple([t[1] for t in data[1]])\n",
    "\n",
    "def getDistance(t1, t2):\n",
    "    import math\n",
    "    assert(len(t1) == len(t2))\n",
    "\n",
    "    distance = 0\n",
    "    for i in range(0, len(t1)):\n",
    "        distance += math.pow(t1[i]-t2[i],2)\n",
    "    \n",
    "    return distance\n",
    "\n",
    "for d in data.collect():\n",
    "    print(getCoordinate(d))\n",
    "    \n",
    "print(\"0-1 distance:\",getDistance(getCoordinate(data.collect()[0]), getCoordinate(data.collect()[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInit(s, n):\n",
    "    import random\n",
    "    random.seed(s)\n",
    "    return random.sample([1, 2, 3, 4, 5],  n)  # Choose 3 elements\n",
    "getInit(1,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inits =  getInit(1,2)\n",
    "print(\"inits\",inits)\n",
    "centroids = data.filter(lambda x: x[0] in inits)\n",
    "print(\"centroids\",centroids.collect())\n",
    "\n",
    "centroids_coordinate = centroids.map(lambda x: getCoordinate(x)).collect()\n",
    "print(\"centroids coordinate\",centroids_coordinate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first iterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCluster(coordinate, centroids):\n",
    "    distance = float(\"inf\")\n",
    "    c = -1\n",
    "    for i in range(0, len(centroids)):\n",
    "        d = getDistance(coordinate, centroids[i])\n",
    "        if(d < distance):\n",
    "            distance = d\n",
    "            c = i\n",
    "    return c\n",
    "\n",
    "def getNewCentroid(clusterRDD):\n",
    "    centroids_coordinate = []\n",
    "    for cluter in clusterRDD:\n",
    "        tuples = []\n",
    "        for c in cluter:\n",
    "            tuples.append(getCoordinate(c))\n",
    "\n",
    "        from numpy import mean\n",
    "        tt = tuple(map(mean, zip(*tuples)))\n",
    "        centroids_coordinate.append(tt)\n",
    "            \n",
    "\n",
    "    return centroids_coordinate\n",
    "\n",
    "def iterate_kmeans(items, centroids):\n",
    "    cluster = []\n",
    "    clusterRDD = []\n",
    "    for i in range(0, len(centroids)):\n",
    "        cluster.append([])\n",
    "        clusterRDD.append([])\n",
    "\n",
    "    for item in items:\n",
    "        coordinate = getCoordinate(item)\n",
    "        clusterIndex = getCluster(coordinate, centroids)\n",
    "        cluster[clusterIndex].append(item[0])\n",
    "        clusterRDD[clusterIndex].append(item)\n",
    "\n",
    "    return cluster, clusterRDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preCluster = None\n",
    "while True: \n",
    "    cluster, clusterRDD = iterate_kmeans(data.collect(), centroids_coordinate)\n",
    "    if cluster == preCluster:\n",
    "        break\n",
    "    else:\n",
    "        print(\"iterate\", cluster)\n",
    "        preCluster = cluster\n",
    "        centroids_coordinate = getNewCentroid(clusterRDD)\n",
    "\n",
    "print(\"finish\", cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
